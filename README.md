# Review of the paper Semantic Answer Similarity for Evaluating Question Answering Models
The evaluation of question answering (QA) models relies on human-annotated datasets of question- answer pairs. Given a question, the ground-truth answer is compared to the answer predicted by a model with regard to different similarity met- rics. Currently, the most prominent metrics for the evaluation of QA models are exact match (EM), F1-score, and top-n-accuracy. All these three met- rics rely on string-based comparison. Even a prediction that differs from the ground truth in only one character in the answer string is evaluated as completely wrong. To mitigate this problem and have a continuous score ranging between 0 and 1, the F1-score can be used. In this case, precision is calculated based on the relative number of tokens in the predicti that are also in the ground-truth answer and recall is calculated based on the relative number of tokens in the ground-truth answer that are also in the prediction.
