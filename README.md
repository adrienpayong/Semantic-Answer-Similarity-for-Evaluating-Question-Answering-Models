The evaluation of question answering (QA) models relies on human-annotated datasets of question- answer pairs. Given a question, the ground-truth answer is compared to the answer predicted by a model with regard to different similarity met- rics. Currently, the most prominent metrics for the evaluation of QA models are exact match (EM), F1-score, and top-n-accuracy. All these three met- rics rely on string-based comparison. Even a prediction that differs from the ground truth in only one character in the answer string is evaluated as completely wrong.
